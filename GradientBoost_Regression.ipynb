{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pentakll4002/boosts/blob/main/GradientBoost_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "623rlRBSgerO"
      },
      "source": [
        "# 1) Problem statement.\n",
        "\n",
        "- This dataset comprises used cars sold on cardekho.com in India as well as important features of these cars.\n",
        "- If user can predict the price of the car based on input features.\n",
        "- Prediction results can be used to give new seller the price suggestion based on market condition.\n",
        "\n",
        "# 2) Data Collection.\n",
        "- The Dataset is collected from scrapping from cardekho website.\n",
        "- The data consists of 13 columns and 15411 rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyjWxV1zgr3e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znKpudcohSH9"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('cardekho_dataset.csv', index_col=[0])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S9WWkZfh57_"
      },
      "source": [
        "# Data Cleaning\n",
        "\n",
        "## Handling Missing Value\n",
        "- Handling Missing value\n",
        "- Handling Duplicated\n",
        "- Check data type\n",
        "- Understanding the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnXNNoF_iF6z"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0jInI5niKFx"
      },
      "outputs": [],
      "source": [
        "## Remove Unnecessary Columns\n",
        "df.drop('car_name', axis=1, inplace=True)\n",
        "df.drop('brand', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZddceGniWd6"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMbjofMliYCl"
      },
      "outputs": [],
      "source": [
        "# Check xem có những phần tử nào trong index model\n",
        "df['model'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPx-27Egin2C"
      },
      "outputs": [],
      "source": [
        "## Getting All Different Types OF Features\n",
        "num_features = [feature for feature in df.columns if df[feature].dtype != 'O']\n",
        "print('Num of Numerical Features :', len(num_features))\n",
        "cat_features = [feature for feature in df.columns if df[feature].dtype == 'O']\n",
        "print('Num of Categorical Features :', len(cat_features))\n",
        "discrete_features=[feature for feature in num_features if len(df[feature].unique())<=25]\n",
        "print('Num of Discrete Features :',len(discrete_features))\n",
        "continuous_features=[feature for feature in num_features if feature not in discrete_features]\n",
        "print('Num of Continuous Features :',len(continuous_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhm6p14ZkC2B"
      },
      "source": [
        "# Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c7aAYE9kEtb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df.drop(['selling_price'], axis=1)\n",
        "y = df['selling_price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4cSmyzckSbX"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYlU7F2GkTEM"
      },
      "outputs": [],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGXwI9d8kfvR"
      },
      "source": [
        "\n",
        "# Feature Encoding and Scaling\n",
        "\n",
        "## One Hot Encoding for Columns which had lesser unique values and not ordinal\n",
        "\n",
        "\n",
        "• One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDGmnY9-k4is"
      },
      "outputs": [],
      "source": [
        "len(df['model'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeGD-Os5lJyh"
      },
      "outputs": [],
      "source": [
        "df['model'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p1on405kr2c"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "X['model'] = le.fit_transform(X['model'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejo0B3mglthE"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnrr6BXImO0c"
      },
      "outputs": [],
      "source": [
        "len(df['seller_type'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRqh00CrmXyU"
      },
      "outputs": [],
      "source": [
        "len(df['fuel_type'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8YnZ8gBmdVD"
      },
      "outputs": [],
      "source": [
        "len(df['transmission_type'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reOnJdBEk9hN"
      },
      "outputs": [],
      "source": [
        "# Create Column Transformer with 3 types of transformers\n",
        "num_features = X.select_dtypes(exclude=\"object\").columns\n",
        "onehot_columns = ['seller_type','fuel_type','transmission_type']\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "numeric_transformer = StandardScaler()\n",
        "oh_transformer = OneHotEncoder(drop='first')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    [\n",
        "        (\"OneHotEncoder\", oh_transformer, onehot_columns),\n",
        "        (\"StandardScaler\", numeric_transformer, num_features)\n",
        "\n",
        "    ],remainder='passthrough'\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnFnSrZwnGea"
      },
      "outputs": [],
      "source": [
        "X = preprocessor.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkcWlUd6ncUv"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RItnGpwmnPTj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VDhBJmynk4-"
      },
      "source": [
        "# Model Training And Model Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOZBgjYbnlpb"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge,Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k29V7kvsnoQl"
      },
      "outputs": [],
      "source": [
        "##Create a Function to Evaluate Model\n",
        "def evaluate_model(true, predicted):\n",
        "    mae = mean_absolute_error(true, predicted)\n",
        "    mse = mean_squared_error(true, predicted)\n",
        "    rmse = np.sqrt(mean_squared_error(true, predicted))\n",
        "    r2_square = r2_score(true, predicted)\n",
        "    return mae, rmse, r2_square"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ziMAXTPnr_V"
      },
      "outputs": [],
      "source": [
        "## Beginning Model Training\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Lasso\": Lasso(),\n",
        "    \"Ridge\": Ridge(),\n",
        "    \"K-Neighbors Regressor\": KNeighborsRegressor(),\n",
        "    \"Decision Tree\": DecisionTreeRegressor(),\n",
        "    \"Random Forest Regressor\": RandomForestRegressor(),\n",
        "    \"Adaboost Regressor\":AdaBoostRegressor(),\n",
        "    \"Graident BoostRegressor\":GradientBoostingRegressor()\n",
        "\n",
        "}\n",
        "\n",
        "for i in range(len(list(models))):\n",
        "    model = list(models.values())[i]\n",
        "    model.fit(X_train, y_train) # Train model\n",
        "\n",
        "    # Make predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate Train and Test dataset\n",
        "    model_train_mae , model_train_rmse, model_train_r2 = evaluate_model(y_train, y_train_pred)\n",
        "\n",
        "    model_test_mae , model_test_rmse, model_test_r2 = evaluate_model(y_test, y_test_pred)\n",
        "\n",
        "\n",
        "    print(list(models.keys())[i])\n",
        "\n",
        "    print('Model performance for Training set')\n",
        "    print(\"- Root Mean Squared Error: {:.4f}\".format(model_train_rmse))\n",
        "    print(\"- Mean Absolute Error: {:.4f}\".format(model_train_mae))\n",
        "    print(\"- R2 Score: {:.4f}\".format(model_train_r2))\n",
        "\n",
        "    print('----------------------------------')\n",
        "\n",
        "    print('Model performance for Test set')\n",
        "    print(\"- Root Mean Squared Error: {:.4f}\".format(model_test_rmse))\n",
        "    print(\"- Mean Absolute Error: {:.4f}\".format(model_test_mae))\n",
        "    print(\"- R2 Score: {:.4f}\".format(model_test_r2))\n",
        "\n",
        "    print('='*35)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRYoCHlGoIx0"
      },
      "outputs": [],
      "source": [
        "#Initialize few parameter for Hyperparamter tuning\n",
        "\n",
        "rf_params = {\"max_depth\": [5, 8, 15, None, 10],\n",
        "             \"max_features\": [5, 7, \"auto\", 8],\n",
        "             \"min_samples_split\": [2, 8, 15, 20],\n",
        "             \"n_estimators\": [100, 200, 500, 1000]}\n",
        "\n",
        "gradient_params={\"loss\": ['squared_error','huber','absolute_error'],\n",
        "             \"criterion\": ['friedman_mse','squared_error','mse'],\n",
        "             \"min_samples_split\": [2, 8, 15, 20],\n",
        "             \"n_estimators\": [100, 200, 500],\n",
        "              \"max_depth\": [5, 8, 15, None, 10],\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xJk0ojtpDnk"
      },
      "outputs": [],
      "source": [
        "# Models list for Hyperparameter tuning\n",
        "randomcv_models = [\n",
        "                   (\"RF\", RandomForestRegressor(), rf_params),\n",
        "                   (\"GradientBoost\",GradientBoostingRegressor(),gradient_params)\n",
        "                   ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da2H44LTpNMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d458c15c-9220-4b80-a140-e7f62897de12"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c0f7ab225e76>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                    n_jobs=-1)\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1952\u001b[0m             ParameterSampler(\n\u001b[1;32m   1953\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## Hyperparameter Tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "model_param = {}\n",
        "for name, model, params in randomcv_models:\n",
        "    random = RandomizedSearchCV(estimator=model,\n",
        "                                   param_distributions=params,\n",
        "                                   n_iter=100,\n",
        "                                   cv=3,\n",
        "                                   verbose=2,\n",
        "                                   n_jobs=-1)\n",
        "    random.fit(X_train, y_train)\n",
        "    model_param[name] = random.best_params_\n",
        "\n",
        "for model_name in model_param:\n",
        "    print(f\"---------------- Best Params for {model_name} -------------------\")\n",
        "    print(model_param[model_name])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Retraining the models with best parameters\n",
        "models = {\n",
        "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, min_samples_split=2, max_features='auto', max_depth=None,\n",
        "                                                     n_jobs=-1),\n",
        "     \"GradientBoost Regressor\":GradientBoostingRegressor(n_estimators= 200,\n",
        "                                                         min_samples_split=8, max_depth=10, loss= 'huber', criterion='mse')\n",
        "\n",
        "}\n",
        "for i in range(len(list(models))):\n",
        "    model = list(models.values())[i]\n",
        "    model.fit(X_train, y_train) # Train model\n",
        "\n",
        "    # Make predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    model_train_mae , model_train_rmse, model_train_r2 = evaluate_model(y_train, y_train_pred)\n",
        "\n",
        "    model_test_mae , model_test_rmse, model_test_r2 = evaluate_model(y_test, y_test_pred)\n",
        "\n",
        "    print(list(models.keys())[i])\n",
        "\n",
        "    print('Model performance for Training set')\n",
        "    print(\"- Root Mean Squared Error: {:.4f}\".format(model_train_rmse))\n",
        "    print(\"- Mean Absolute Error: {:.4f}\".format(model_train_mae))\n",
        "    print(\"- R2 Score: {:.4f}\".format(model_train_r2))\n",
        "\n",
        "    print('----------------------------------')\n",
        "\n",
        "    print('Model performance for Test set')\n",
        "    print(\"- Root Mean Squared Error: {:.4f}\".format(model_test_rmse))\n",
        "    print(\"- Mean Absolute Error: {:.4f}\".format(model_test_mae))\n",
        "    print(\"- R2 Score: {:.4f}\".format(model_test_r2))\n",
        "\n",
        "    print('='*35)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "EbQDWnhnNfAW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPq8X+uGVMU9c7tKscCa0ky",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}